{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a9f781",
   "metadata": {},
   "source": [
    "# Interest-based Post Recommender (Hybrid: Content + CF)\n",
    "\n",
    "_A clean, Colab-style notebook you can run end-to-end._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Imports & Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix, vstack as sp_vstack\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Load Data\n",
    "# Adjust the paths below if running outside this environment.\n",
    "USERS_PATH = \"/mnt/data/Users.csv\"\n",
    "POSTS_PATH = \"/mnt/data/Posts.csv\"\n",
    "ENG_PATH   = \"/mnt/data/Engagements.csv\"\n",
    "\n",
    "users = pd.read_csv(USERS_PATH)\n",
    "posts = pd.read_csv(POSTS_PATH)\n",
    "eng   = pd.read_csv(ENG_PATH)\n",
    "\n",
    "users.head(), posts.head(), eng.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ce288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Quick EDA\n",
    "print(\"Users:\", users.shape, list(users.columns))\n",
    "print(\"Posts:\", posts.shape, list(posts.columns))\n",
    "print(\"Engagements:\", eng.shape, list(eng.columns))\n",
    "\n",
    "# Basic distributions\n",
    "print(\"\\nUnique users:\", users['user_id'].nunique())\n",
    "print(\"Unique posts:\", posts['post_id'].nunique())\n",
    "print(\"Unique creators:\", posts['creator_id'].nunique())\n",
    "\n",
    "# Plot engagement distribution (if numeric/binary)\n",
    "if 'engagement' in eng.columns:\n",
    "    eng['engagement'].value_counts(dropna=False).sort_index().plot(kind=\"bar\")\n",
    "    plt.title(\"Engagement Value Counts\")\n",
    "    plt.xlabel(\"engagement\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e11f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Preprocess: Split Interests, Normalize Post Text\n",
    "\n",
    "# Split and clean interests\n",
    "if \"top_3_interests\" in users.columns:\n",
    "    split_cols = users[\"top_3_interests\"].fillna(\"\").astype(str).str.split(\",\", expand=True)\n",
    "    # keep up to 3 columns\n",
    "    split_cols = split_cols.iloc[:, :3] if split_cols.shape[1] >= 3 else split_cols.reindex(columns=[0,1,2])\n",
    "    split_cols = split_cols.rename(columns={0:\"interest_1\",1:\"interest_2\",2:\"interest_3\"})\n",
    "    for c in [\"interest_1\",\"interest_2\",\"interest_3\"]:\n",
    "        if c in split_cols.columns:\n",
    "            split_cols[c] = split_cols[c].fillna(\"\").astype(str).str.strip()\n",
    "        else:\n",
    "            split_cols[c] = \"\"\n",
    "    users = pd.concat([users, split_cols], axis=1)\n",
    "\n",
    "# Normalize posts text from content_type + tags\n",
    "posts[\"tags\"] = posts[\"tags\"].fillna(\"\").astype(str)\n",
    "posts[\"content_type\"] = posts[\"content_type\"].fillna(\"\").astype(str)\n",
    "posts[\"post_text\"] = posts[\"content_type\"].str.replace(r\"[_\\-]\", \" \", regex=True) + \" \" + posts[\"tags\"].str.replace(\",\", \" \")\n",
    "posts[[\"post_id\",\"post_text\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ac91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## TF-IDF for Content-Based Similarity\n",
    "tfidf = TfidfVectorizer(lowercase=True, token_pattern=r\"(?u)\\b\\w+\\b\", ngram_range=(1,2), min_df=1)\n",
    "post_tfidf = tfidf.fit_transform(posts[\"post_text\"])\n",
    "post_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72afdddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Build User Content Profiles (Interests + History)\n",
    "\n",
    "def interests_to_vector(row):\n",
    "    # Join up to three interests\n",
    "    interests = \" \".join([str(row.get(\"interest_1\",\"\")), str(row.get(\"interest_2\",\"\")), str(row.get(\"interest_3\",\"\"))]).strip()\n",
    "    return tfidf.transform([interests])\n",
    "\n",
    "# Interest vectors\n",
    "user_interest_vecs = [interests_to_vector(row) for _, row in users.iterrows()]\n",
    "user_interest_mat = sp_vstack(user_interest_vecs) if len(user_interest_vecs) else csr_matrix((0, post_tfidf.shape[1]))\n",
    "\n",
    "# Map IDs to indices\n",
    "user_ids = users[\"user_id\"].astype(str).tolist()\n",
    "post_ids = posts[\"post_id\"].astype(str).tolist()\n",
    "u_index = {u:i for i,u in enumerate(user_ids)}\n",
    "p_index = {p:i for i,p in enumerate(post_ids)}\n",
    "\n",
    "# Ensure types in engagements\n",
    "eng = eng.copy()\n",
    "eng[\"user_id\"] = eng[\"user_id\"].astype(str)\n",
    "eng[\"post_id\"] = eng[\"post_id\"].astype(str)\n",
    "\n",
    "# Keep only known users/posts in engagements\n",
    "eng = eng[eng[\"user_id\"].isin(u_index) & eng[\"post_id\"].isin(p_index)]\n",
    "\n",
    "# Build a history-weighted user profile from engaged posts\n",
    "num_users = len(u_index)\n",
    "user_history_mat = csr_matrix((num_users, post_tfidf.shape[1]))\n",
    "\n",
    "if eng.shape[0] > 0:\n",
    "    grouped = eng.groupby(\"user_id\")\n",
    "    rows, mats = [], []\n",
    "    for uid, grp in grouped:\n",
    "        ridx = u_index[uid]\n",
    "        pidx = grp[\"post_id\"].map(p_index).values\n",
    "        weights = grp[\"engagement\"].astype(float).values\n",
    "        wsum = weights.sum()\n",
    "        if wsum <= 0 or len(pidx) == 0:\n",
    "            continue\n",
    "        # Weighted average of post vectors\n",
    "        user_vec = (post_tfidf[pidx].multiply(weights[:, None])).sum(axis=0) / wsum\n",
    "        rows.append(ridx)\n",
    "        mats.append(csr_matrix(user_vec))\n",
    "    if mats:\n",
    "        stacked = sp_vstack(mats)\n",
    "        user_history_mat = csr_matrix(user_history_mat, copy=True)\n",
    "        user_history_mat[rows, :] = stacked\n",
    "\n",
    "alpha = 0.6  # interests vs history weight\n",
    "user_content_vec = (alpha * user_interest_mat) + ((1 - alpha) * user_history_mat)\n",
    "user_content_vec.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6506b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Collaborative Filtering via SVD\n",
    "\n",
    "num_items = len(p_index)\n",
    "row_idx = eng[\"user_id\"].map(u_index).values\n",
    "col_idx = eng[\"post_id\"].map(p_index).values\n",
    "data_vals = eng[\"engagement\"].astype(float).values\n",
    "\n",
    "ui = csr_matrix((data_vals, (row_idx, col_idx)), shape=(num_users, num_items))\n",
    "\n",
    "k = min(20, min(num_users, num_items) - 1) if min(num_users, num_items) > 2 else 2\n",
    "svd = TruncatedSVD(n_components=k, random_state=42)\n",
    "user_f = svd.fit_transform(ui)   # U * Sigma\n",
    "item_f = svd.components_.T       # V\n",
    "cf_scores = np.dot(user_f, item_f.T)  # (num_users x num_items)\n",
    "cf_scores.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a6a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Content-Based Scores (Cosine Similarity)\n",
    "content_scores = cosine_similarity(user_content_vec, post_tfidf)  # (num_users x num_items)\n",
    "content_scores.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1799e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Hybrid Scores & Top-3 Recommendations\n",
    "lambda_hybrid = 0.5  # blend CF and content\n",
    "hybrid_scores = lambda_hybrid * content_scores + (1 - lambda_hybrid) * cf_scores\n",
    "\n",
    "# Exclude already positively engaged posts\n",
    "already = eng[eng[\"engagement\"] > 0].groupby(\"user_id\")[\"post_id\"].apply(set).to_dict()\n",
    "\n",
    "top_k = 3\n",
    "recs = []\n",
    "for uid in user_ids:\n",
    "    uidx = u_index[uid]\n",
    "    scores = hybrid_scores[uidx]\n",
    "    exclude = already.get(uid, set())\n",
    "    order = np.argsort(-scores)  # descending\n",
    "    picks, pick_scores = [], []\n",
    "    for j in order:\n",
    "        pid = post_ids[j]\n",
    "        if pid in exclude:\n",
    "            continue\n",
    "        picks.append(pid)\n",
    "        pick_scores.append(float(scores[j]))\n",
    "        if len(picks) == top_k:\n",
    "            break\n",
    "    while len(picks) < top_k:\n",
    "        picks.append(None)\n",
    "        pick_scores.append(None)\n",
    "    recs.append({\n",
    "        \"user_id\": uid,\n",
    "        \"rec_1\": picks[0], \"score_1\": pick_scores[0],\n",
    "        \"rec_2\": picks[1], \"score_2\": pick_scores[1],\n",
    "        \"rec_3\": picks[2], \"score_3\": pick_scores[2],\n",
    "    })\n",
    "\n",
    "recs_df = pd.DataFrame(recs)\n",
    "recs_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f036492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Quick (Simulated) Validation: Hit-Rate@3\n",
    "np.random.seed(42)\n",
    "pos = eng[eng[\"engagement\"] > 0].copy()\n",
    "if len(pos) > 0:\n",
    "    pos[\"rand\"] = np.random.rand(len(pos))\n",
    "    holdout = pos[pos[\"rand\"] < 0.1]\n",
    "    holdout_map = holdout.groupby(\"user_id\")[\"post_id\"].apply(set).to_dict()\n",
    "\n",
    "    hits, total = 0, 0\n",
    "    for uid, items in holdout_map.items():\n",
    "        rec_row = recs_df[recs_df[\"user_id\"] == uid]\n",
    "        if rec_row.empty:\n",
    "            continue\n",
    "        recs_set = set(rec_row[[\"rec_1\",\"rec_2\",\"rec_3\"]].values.flatten().tolist())\n",
    "        recs_set.discard(None)\n",
    "        total += 1\n",
    "        if len(recs_set & items) > 0:\n",
    "            hits += 1\n",
    "\n",
    "    hit_rate_at3 = (hits / total) if total > 0 else np.nan\n",
    "else:\n",
    "    hit_rate_at3 = np.nan\n",
    "\n",
    "print(\"Hit-Rate@3:\", hit_rate_at3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaddebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Save Outputs (Recommendations CSV + Short Report)\n",
    "recs_path = \"/mnt/data/recommendations.csv\"\n",
    "recs_df.to_csv(recs_path, index=False)\n",
    "print(\"Saved:\", recs_path)\n",
    "\n",
    "report = f\"\"\"# Hybrid Recommender (Content + CF)\n",
    "\n",
    "**Data**: Users (n={len(users)}), Posts (n={len(posts)}), Engagements (n={len(eng)}).\n",
    "\n",
    "## Method\n",
    "1. **Content-based**:\n",
    "   - TF-IDF over `content_type + tags` for each post.\n",
    "   - User profile = 0.6 × TF-IDF(interests) + 0.4 × TF-IDF(history-weighted).\n",
    "   - Score = cosine similarity between user profile and post vectors.\n",
    "\n",
    "2. **Collaborative Filtering**:\n",
    "   - User–Item matrix from `engagement`.\n",
    "   - Low-rank factors via TruncatedSVD (k={int(min(20, min(len(users), len(posts)) - 1)) if min(len(users), len(posts)) > 2 else 2}).\n",
    "   - Score = reconstructed dot product between user and item factors.\n",
    "\n",
    "3. **Hybrid**:\n",
    "   - Final score = 0.5 × Content + 0.5 × CF.\n",
    "   - Exclude posts already positively engaged by the user.\n",
    "   - Return Top-3 per user.\n",
    "\n",
    "## Quick Validation (simulated)\n",
    "- 10% random holdout of positive interactions per user (no timestamps available).\n",
    "- **Hit-Rate@3** = {hit_rate_at3:.3f} (nan if no positives/holdouts).\n",
    "\n",
    "## Notes & Extensions\n",
    "- If timestamps exist, do a **time-based split** to avoid leakage.\n",
    "- Tune weights `alpha` (interests vs history) and `lambda` (content vs CF) by validation.\n",
    "- Replace TF-IDF with **SentenceTransformers** if post text is richer.\n",
    "- Replace SVD with **implicit ALS** or **Neural CF** for stronger collaborative signal.\n",
    "- Add **diversity** constraints (MMR) and **freshness** boosts for production.\n",
    "\"\"\"\n",
    "\n",
    "report_path = \"/mnt/data/report.md\"\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(report)\n",
    "print(\"Saved:\", report_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaa050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Next Steps\n",
    "# - Use time-based split if timestamps exist to avoid leakage.\n",
    "# - Hyperparameter tuning for `alpha` and `lambda_hybrid`.\n",
    "# - Consider implicit ALS (e.g., `implicit` library) or Neural CF for stronger CF.\n",
    "# - Use SentenceTransformers for richer text embeddings if post descriptions are long.\n",
    "# - Add re-ranking for diversity and freshness.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
